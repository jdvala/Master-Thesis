\chapter{Concept}
\label{ch:concept}

This chapter describes in detail the conceptual aspects of this thesis. The concept is divided into three phases, in the first phase data collection and cleaning aspects are described, in the next phase clustering, and resampling of the data is described, and in the last phase, description of machine learning and deep learning models with their evaluation strategies are described.

\section{Phase 1: Data Collection and Cleaning}
In the first phase, the data collection techniques are described in detail, also how the data was cleaned to make it suitable for the machine learning algorithm is described here. 

\subsection*{Data Collection}
Legal laws are written to be applicable in various situations. For example, a law for agricultural products may be applicable in cases where trading of goods happens.  High-level categories of multilingual legal documents provided by the European Union help to navigate in the legislative corpus. They are also comprised of many languages so that is can be used by various Nations. The applicability of these legal texts in different situations makes the classification of these texts on the multi-label classification problem.

There are many datasets available for legal domains. CFPB Credit Card Agreements DB\footnote{http://www.consumerfinance.gov/credit-cards/agreements/}, EUR-Lex Dataset\footnote{http://www.ke.tu-darmstadt.de/resources/eurlex} are some well known datasets for legal domain-specific tasks. However, the CFPB credit card agreements database is not available in the German language and also it is about one specific topic in the legal domain. The EUR-Lex Dataset is a large dataset available in both English and German, which was scraped from Publication's office of European Union's website \cite{mencia2010efficient}. This dataset consists of 19,348 documents for a single language divided into 201 topics. This dataset was too large to handle as there are 201 classes and given the amount of resources available to scrap, process and train the models. So instead EUR-Lex summaries are selected. This EUR-Lex Summaries\footnote{https://eur-lex.europa.eu/browse/summaries.html} are also from the Publication office of European Union's Parliament. These summaries are the European Union's laws explained and translated into reader-friendly language. They cover 32 topics which correspond to the activity areas of the European Union. These summaries are available in multiple languages. The EUR-Lex summaries dataset is highly imbalanced data. For this thesis, only \textit{English and German} languages are considered due to their availability across most documents and also the amount of resource available for processing and training the machine learning algorithms were limited. The summaries are not available in an easy click to download manner, and hence it has to be scraped off of the European Union's publication office's website. These summaries frequently change as the laws of the union changes or if a new law is introduced and as stated above these summaries might belong to multiple classes. 

While scraping the summaries, data from both languages need to be scraped together. This ensures that we are not missing out any summary from either of the languages. Saving the summaries from different languages separately would create the problem of data leaking, which is the result of splitting data randomly for training and testing. For example, \textbf{Doc A} which belongs to \textbf{class 1} in English and \textbf{Doc B} which is the same document but in German which also belong to \textbf{class 1}. Now, when spitting the data for training and testing \textbf{Doc A} might end up in the training set, and \textbf{Doc B} might end up in the testing set. 




\subsection*{Data Cleaning}\label{ConceptCleaning}

Text data is unstructured and noisy, meaning that it is not organized in a predefined manner and noisy because it contains a lot of information that may not be of any use for the task at hand. These irregularities make it difficult for a machine learning algorithm to learn from it. Textual data is non-uniform as the structure and content changes from one document to others, due to this non-uniformity it is challenging to have a single cleaning mechanism for all machine learning problems involving text. For example, a machine learning task that is designed to detect events might need date and time to effectively predict the event, whereas it would be viable to remove them for a sentiment analysis task.

Legal text is different from other text available because of the style, structure \cite{boella2011using} and the use of special characters which is exclusive to the legal text. General text preprocessing techniques becomes feeble, hence during preprocessing of legal text special care must be taken. The data set used here is the summarised version of the original legislation, which contains links to the original documents as well as background information about the law and other information that might not contribute anything in the classification process. These chunks of information can be removed, but it is particularly hard to remove them as they do not follow any pattern to where they appear in the document. Due to this reason, even though this information will increase the training time, it was not removed due to its demanding effort.

\section{Phase 2: Data Clustering and Resampling}
%\todo{have to mention clustering first and then resampling}
In the second phase, the data is first resampled and then clustered to prepare the data for training in the next phase. Also, in this phase, the word embeddings for training are created.

\subsection*{Data Resampling}

Data for almost all the real world application is in skewed distribution, and EUR-Lex summaries are no exception. Skewed distribution is when we have more samples in one class then others \cite{wang2012multiclass}. Skewed distribution also known as class imbalance in data is a difficult situation for a classifier to learn from as the classifier are biased towards the majority classes and shows poor performance on minority classes. Also, in the case of class imbalance in the dataset, accuracy is not a good performance evaluation measure \cite{chawla2002smote}.  

To handle the imbalances in the dataset various techniques have been proposed. \textit{Oversampling} and \textit{Undersampling} are the two most common ones. Oversampling refers to the process of artificially increasing the number of samples in the minority classes, and Undersampling is when we remove instances from the majority class to even out the dataset. Undersampling in our case would mean that we reduce the instances from the majority classes which are as it less to being with. And I could not find any work of oversampling domain-specific and limited textual dataset using techniques like SMOTE \cite{chawla2002smote}.   

The EUR-Lex summaries are multi-label; this is generally the case with legal text as a single case may be applicable in many situations. So having multiple labels for a single document can be exploited to alleviate the skewness in the dataset. The sampling of the documents is done in such a way that it reduces samples from the majority classes on the bases of its presence in the minority classes. Hence, a document will be labeled with the class with fewest examples. 

The resampling technique can be better understood with the following example. Consider the dataset in the \ref{table:exampleDataResampling} and the distribution of samples across classes in \ref{table:SamplesDistributionDataResamplingExample}

\begin{table}[!ht]
\centering
\begin{tabular}{cc}

\hline
\textbf{Doc ID} & \textbf{Class Label} \\ \hline
Document A           & 1,5,2                \\ 
Document B           & 3,2,5                    \\ 
Document C           & 4,1,5                  \\ 
Document D           & 2,3                    \\ 
Document E           & 2,5                  \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{An table showing documents and their assignments in their respective classes}
\label{table:exampleDataResampling}
\end{table}
\clearpage

\begin{table}[!ht]
\centering
\begin{tabular}{cc}
\hline
\textbf{Class ID} & \textbf{Number of Samples} \\ \hline
1                 & 2                         \\ 
2                 & 4                          \\ 
3                 & 1                          \\ 
4                 & 1                          \\ 
5                 & 4                         \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Distribution of samples in the dataset across 5 classes}
\label{table:SamplesDistributionDataResamplingExample}
\end{table}

When applying the proposed resampling technique to the dataset in \ref{table:exampleDataResampling} and  \ref{table:SamplesDistributionDataResamplingExample}, $Document$ $A$ which has label 1,5 and 2 will be assigned class 1 because the number of samples in class 1 are less then number of samples in class 2 and class 5, similarly $Document$ $B$ will be assigned to class 3, $Document$ $C$  to class 4, $Document$ $D$ to class 2 and $Document$ $E$ to either 2 or 5. \ref{table:exampleDataResampling} will become

\begin{table}[!ht]
\centering
\begin{tabular}{cc}

\hline
\textbf{Doc ID} & \textbf{Class Label} \\ \hline
Document A           & 1                \\ 
Document B           & 3                    \\ 
Document C           & 4                  \\ 
Document D           & 2                   \\ 
Document E           & 2 or 5                  \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Document assignment after proposed resampling}
\label{table:exampleDataResamplingAfterResampling}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{cc}
\hline
\textbf{Class ID} & \textbf{Number of Samples} \\ \hline
1                 & 1                        \\ 
2                 & 1 or 2                          \\ 
3                 & 1                          \\ 
4                 & 1                          \\ 
5                 & 0 or 1                         \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Distribution of samples in the dataset across 5 classes after resampling}
\label{table:SamplesDistributionDataResamplingExampleAfterResampling}
\end{table}

\todo{Should this be here?}
This method of resampling has its own limitation. As the samples from the majority class are removed and only kept in minority class, the majority class is losing information about that sample. If the data in a class is diverse, that is, if every sample of that class is unique then applying this technique would result in loss of that unique representation of the class.

\clearpage

\subsection*{Data Clustering}\label{concpeClustering}
After resampling, the data is divided into $n$ groups. To find the number of groups that the data must be divided into, Silhouette Score and Elbow Analysis are performed as described in \ref{chooseK}. The division is so that all documents $d$ of a category $C_{l}$ will be grouped together in the same cluster $K_{n}$ via must-link constraints.

%\begin{align}
   % C_{t} = U_{S\in I_{l}}d_{S} \Rightarrow \exists!K_{n}  \forall_{S} \in I_{S} : d_{S} \in K_{n}
%\end{align}

That is, each category $C_{l}$ consisting of documents $d_{s}$ among all other instances with the corresponding class labels $I_{l}$, hence there is only one cluster $K_{n}$ which will have all the instances of the same class (must-link constraints). Each document will be assigned to the closest cluster without violating the must-link constraints. To achieve this, a constrained version of k-means algorithm is applied (see \ref{constrainedKMeans}). Only must-link constraints are available to us in the form of class labels, as each document will belong to one of the 32 classes, however cannot-link constraints, can not be obtained as we do not possess this information or have any prior knowledge about which two documents will not belong together. 

When splitting the data in this manner, the number of classes to be predicted by each multi-class classifier of a cluster is reduced, thus resulting in specialization advantages for each classifier. However, the final classification depends on the results of the previous steps. Each classifier in the chain introduces errors and has less data for training. The classification problem is changed because each classifier has only a subset of classes to predict. Even though the amount of training instances is reduced, the performance effect can still be higher compared to a single multi-class classifier.



\subsection*{Monolingual and Bilingual Word Embeddings}
Training the word embeddings is the last step of the second phase. Three sets of word embeddings, one monolingual word embedding, one bilingual word embedding trained on legal data and another bilingual word embedding trained on general-purpose data.   

The details of the training the word embeddings will be discussed in the next chapter.

\section{Phase 3: Model Architecture and Training}\label{sec:conceptRQ}
The third phase goes into details regarding the conceptual aspect of the three research questions. different evaluation strategies for comparing them. 


\subsection{Research Question 1:} \label{question1}
The first research question is:
\begin{quote}
    Can \textit{\gls{BiLSTM}} achieve better results in terms of evaluation metrics (Precision, Recall, and F-Score) than the thoroughly studied text classification methods such as \textit{Support Vector Machines}?
\end{quote}

\ref{fig:FlowResearchQuestion1} shows the workflow for the above-mentioned research question. After acquiring the data from the EUR-Lex website, the data was cleaned or prepossessed (see \ref{ConceptCleaning}). Upon preprocessing, the textual data needs to be converted using suitable text representations technique according to the algorithm it will be trained on. Here, there are two algorithms being tested, \gls{SVM} and \gls{BiLSTM}, and both of them employ different techniques to represent text for training (See \ref{backgroundTextRepresentation}). The \gls{SVM} uses the \gls{TF-IDF} representation whereas the LSTM uses the Word Embeddings. 
 
Due to hardware limitations training the \gls{BiLSTM} is difficult if the length of a single instance is very long. Hence each document is divided into sentences using sliding window technique (See \ref{backgroundSlidingWindow}) and document label was assigned to each sentence created from that document. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=15cm, height=14cm,keepaspectratio]{pics/flowforQuestion1.jpeg}
    \captionsetup{justification=centering,margin=1cm}
    \caption{Flow chart representing the workflow for the first research question }
    \label{fig:FlowResearchQuestion1}
\end{figure}

To see the effect of clustering on the data for classification purpose, one classification model will be trained on clustered data in a hierarchical manner. The clustered data will contain documents from corpora of both the languages; this is because clustering on a corpus of a single language will result in very less data for the clusters. This will show us if there is any improvement in classification performance of the classifier with the specialization advantage discussed in \ref{concpeClustering}. The training workflow for the \gls{BiLSTM} (with clustering) is shown below in \ref{fig:clusterFlowClassification}, 
\clearpage
\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm,keepaspectratio]{pics/approach_cls.png}
    \captionsetup{justification=centering,margin=1cm}
    \caption{Classification workflow for clustered data in hierarchical manner}
    \label{fig:clusterFlowClassification}
\end{figure}

\subsubsection*{Evaluation} \label{evaluationQuestionOne}
The \gls{SVM} is trained on full documents, and the \gls{BiLSTM} is trained on the sentences created using the sliding window technique, for this reason, the results from both the models cannot be compared. Ferguson et al.,(2009) \cite{ferguson2009exploring} performed sentiment analysis of the financial blogs on paragraph-level 
annotated text, they summed up the predictions given by the classifier. Similarly, in our case, the classifier will give probability to each sentence belonging to each class. As each document comprises of sentences, if we add all the probabilities for each class across all the sentences of a document we get probabilities for every class for that document, and then we assign the class with highest probability score to the document. The classification algorithm trained on clustered data would yield two different sets of predictions, i.e., one for the first cluster and one for the second cluster, the scores for both the clusters will then be averaged to get the score on the whole dataset.

\clearpage
The \ref{table:ListQuestionFirstQuestion} lists all the details of the classification algorithms used for the first research question,


\begin{table}[!ht]
\centering
\begin{tabular}{>{\centering\arraybackslash}m{3cm}>{\centering\arraybackslash}m{3cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{2.3cm}>{\centering\arraybackslash}m{3cm}}
\hline
\textbf{Classification Algorithm} & \textbf{Text Representation} & \textbf{Corpus} & \textbf{Type of training} & \textbf{Evaluation Strategy} \\ [0.2cm]\hline
\gls{SVM} & \gls{TF-IDF} & English & Non Clustered & Document Level \\[0.3cm]
\gls{BiLSTM} & Monolingual Word Embeddings (Legal Text) & English & Non Clustered & Document Level and Sentence Level \\[0.3cm]
\gls{BiLSTM} & Bilingual Word Embedding (Legal Text) & English and German & Non Clustered & Document Level and Sentence Level \\[0.3cm]
\gls{BiLSTM} & Bilingual Word Embedding (Legal Text) & English and German & Clustered & Document Level and Sentence Level \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Classification algorithms for the first research question with their corresponding text representation techniques, the corpus used to train them, the type of training (clustered or non clustered) and the evaluation strategies}
\label{table:ListQuestionFirstQuestion}
\end{table}



\subsection{Research Question 2:} \label{question2}
The second research question is:

\begin{quote}
    Are general-purpose resources such as \textit{pre-trained word embeddings} in case of \textit{LSTM} applicable to specific legal domain tasks in terms of evaluation metrics? Also, further training them on the legal corpus, produces comparable results to the ones only trained on legal texts?
\end{quote}



\ref{fig:FlowResearchQuestion2} shows the general workflow for the second question. For this question we compare the general-purpose resources, that is word embeddings trained on textual data available from many different domains, with the word embeddings trained on the legal corpus. 

As the domain-specific word embeddings are created using legal domain-specific data this word embedding will not be trained further. In case of general-purpose word embeddings, two models will be trained - one with frozen (embedding weights will not be updated during the training process) embedding layer and other models will train the word embeddings. This is done to see if further training the general-purpose word embeddings produces results comparable to the ones trained on legal data.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=15cm, height=14cm,keepaspectratio]{pics/flowforQuestion2.jpeg}
    \captionsetup{justification=centering,margin=1cm}
    \caption{Flow chart representing the workflow for the second research question}
    \label{fig:FlowResearchQuestion2}
\end{figure}

\subsection*{Evaluation}

Similar to the first question, the classification models will be trained on sentences for English and German corpus, hence all the models will be evaluated on document level and sentence level with micro-average precision, recall and f1-score performance matrices. All the classification models will be trained on clustered data in the similar way as shown in the  \ref{fig:clusterFlowClassification}. As the training is done on the clustered data, the scores of the performance matrices will be averaged together similar to the evaluation done in research question 1 (see \ref{evaluationQuestionOne}).
\clearpage
The \ref{table:ListQuestionSecondQuestion} below lists all the classification algorithms for the second research question,
\begin{table}[!ht]
\begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2.9cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1.9cm}>{\centering\arraybackslash}m{2cm}}
\hline
\textbf{Classification Algorithm} & \textbf{Word Embeddings} & \textbf{Embedding Training} & \textbf{Corpus} & \textbf{Type of Training} & \textbf{Evaluation Strategy} \\ \hline
\gls{BiLSTM} & Domain-Specific Bilingual (Legal Text) & False & English and German & Non Clustered & Document Level and Sentence Level \\ [0.2cm]
\gls{BiLSTM} & General-Purpose Bilingual (Facebook's MUSE) & False & English and German & Non Clustered & Document Level and Sentence Level \\ [0.2cm]
\gls{BiLSTM} & General-Purpose Bilingual (Facebook's MUSE) & True & English and German & Clustered & Document Level and Sentence Level \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Classification algorithms for the second research question with their corresponding text representation techniques, the corpus used to train them, the type of training (clustered or non clustered) and the evaluation strategies}
\label{table:ListQuestionSecondQuestion}
\end{table}

\subsection{Research Question 3:} \label{question3}

The third research question is:

\begin{quote}
    Can \textit{\gls{BiLSTM}} perform better when training multiple languages in a single model, compared to training one model for each language separately?
\end{quote}

\ref{fig:FlowResearchQuestion3} shows the general workflow for the third research question. In this question, we explore the possible effects of having data in multiple languages on a classifier. C.-P. Wei et al.(2014)\cite{Wei:2014:EPD:2566999.2567111} suggested that when the training set for a single language is small it is less representative of the target semantic space and a classifier trained on this data set would not yield satisfactory results. However, if the text from another language is available for the predefined category then it can be beneficial and can improve the effectiveness of the classifier for the previous language. The naïve approaches of handling such data is to build separate classifiers for separate languages. This naïve method does not take the benefit of the availability of data across different language for a defined category. 

To see the effect of the advantage of multilingual data, in this research question, first, two separate classifiers will be trained on two different languages; namely, \textit{English} and \textit{German}. The results from both of them will be combined to get the average performance (we will average the macro-average precision, recall, and f-score across both the models) of classification of these two classifiers. The training and evaluation of these classifiers will be similar to the training done in research question 1 and 2. This performance of the two classifiers will then be compared to a single classifier that will be trained on both the languages with multilingual word embeddings.

The training of all the classifiers for this question will be done in the similar fashion as shown in \ref{fig:clusterFlowClassification}, and \ref{table:ListQuestionThirdQuestion} below lists all the classification algorithms for the third research question,


\begin{figure}[!ht]
    \centering
    \includegraphics[width=15cm, height=14cm,keepaspectratio]{pics/flowforQuestion3.jpeg}
    \captionsetup{justification=centering,margin=1cm}
    \caption{Flow chart representing the workflow for the third research question}
    \label{fig:FlowResearchQuestion3}
\end{figure}

\clearpage


\subsection*{Evaluation}

For the evaluation, the multilingual classifier is evaluated on the summation of predictions from the sentences of each document for each class. Also, both of the monolingual classifiers will be evaluated on summation of the predictions from the sentences of each document of each class but these results will then be averaged to be able to compare it with the multilingual classifier also to see how they perform on sentence level the evaluation will also consider the results on sentence level.

\begin{table}[!ht]
\begin{tabular}{>{\centering\arraybackslash}m{2.9cm}>{\centering\arraybackslash}m{2.9cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2.6cm}}
\hline
\textbf{Classification Algorithm} & \textbf{Word Embeddings} & \textbf{Corpus} & \textbf{Type of training} & \textbf{Evaluation Strategy} \\ \hline
\gls{BiLSTM} & Monolingual Domain-Specific (Legal Text) & English & Non Clustered & Document Level \&  Sentence Level \\ [0.2cm]
\gls{BiLSTM} & Monolingual Domain-Specific (Legal Text) & German & Non Clustered & Document Level \&  Sentence Level \\[0.2cm]

\gls{BiLSTM} & Bilingual Domain-Specific (Legal Text) & English and German & Clustered & Document Level \&  Sentence Level \\ \hline
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\caption{Classification algorithms for the third research question with their corresponding text representation techniques, the corpus used to train them, the type of training (clustered or non clustered ) and the evaluation strategies}
\label{table:ListQuestionThirdQuestion}
\end{table}